import numpy as np
import pandas as pd
from itertools import chain
from tifffile import imread
from skimage.filters import threshold_otsu
import cv2
import optuna

from twocan import preprocess_if, preprocess_imc, multi_channel_corr, RegEstimator, prep_zarr
from twocan.callbacks import SaveTrialsDFCallback


# List of attributes to set as NaN when trial fails
df_na_list = [
    'registration_matrix','prop_source_covered', 'prop_target_covered', 
    'logical_and', 'logical_xor','logical_iou',
    'stack_image_max_corr','reg_image_max_corr','corr_image_max_corr',
    'stack_cell_max_corr','reg_cell_max_corr','corr_cell_max_corr'
]

def registration_trial(trial, images, if_scale, registration_channels, correlation_channels):
    """Run a single registration trial with the given parameters.
    
    This function handles the optimization of registration parameters and computes
    various metrics for evaluating the registration quality.
    """
    # Set up trial parameters
    trial.suggest_float("IF_binarization_threshold", 0, 1)
    trial.suggest_float("IF_gaussian_sigma", 0, 5)
    trial.suggest_categorical("IMC_arcsinh_normalize", [True, False])
    trial.suggest_float("IMC_arcsinh_cofactor", 1, 100)
    trial.suggest_float("IMC_winsorization_lower_limit", 0, 0.2)
    trial.suggest_float("IMC_winsorization_upper_limit", 0, 0.2)
    trial.suggest_float("IMC_binarization_threshold", 0, 1)
    trial.suggest_float("IMC_gaussian_sigma", 0, 5)
    trial.suggest_categorical("binarize_images", [True])
    trial.suggest_categorical("registration_max_features", [int(1e5)])
    trial.suggest_categorical("registration_percentile", [0.9])
    trial.suggest_categorical("registration_target", ['IF'])
    
    # Extract arrays and channels
    IF = images['IF'].to_numpy()
    IMC = images['IMC'].to_numpy()
    IF_reg = IF[images['IF'].c.to_index().isin(registration_channels)]
    IMC_reg = IMC[images['IMC'].c.to_index().isin(registration_channels)]
    IF_corr = IF[images['IF'].c.to_index().isin(correlation_channels)]
    IMC_corr = IMC[images['IMC'].c.to_index().isin(correlation_channels)]   
    
    # Preprocess images
    IF_processed = preprocess_if(IF_reg, if_scale, trial.params["binarize_images"], 
                               trial.params["IF_binarization_threshold"], 
                               trial.params["IF_gaussian_sigma"])
    IMC_processed = preprocess_imc(IMC_reg, trial.params["IMC_arcsinh_normalize"], 
                                 trial.params["IMC_arcsinh_cofactor"],
                                 [trial.params["IMC_winsorization_lower_limit"], 
                                  trial.params["IMC_winsorization_upper_limit"]], 
                                 trial.params["binarize_images"],
                                 trial.params["IMC_binarization_threshold"], 
                                 trial.params["IMC_gaussian_sigma"])
    
    # Check for invalid preprocessing results
    if (IF_processed).all() or (~IF_processed).all(): 
        [trial.set_user_attr(k, np.NaN) for k in df_na_list]
        return
    if (IMC_processed).all() or (~IMC_processed).all(): 
        [trial.set_user_attr(k, np.NaN) for k in df_na_list]
        return
    
    # Register images
    if trial.params["registration_target"] == 'IF':
        reg = RegEstimator(trial.params["registration_max_features"], trial.params["registration_percentile"])
        try:
            reg.fit(IMC_processed, IF_processed)
        except cv2.error:
            [trial.set_user_attr(k, np.NaN) for k in df_na_list]
            return
            
        # Check for invalid registration results
        if (reg.M_ is None) or (np.linalg.det(reg.M_[0:2,0:2]) == 0):
            [trial.set_user_attr(k, np.NaN) for k in df_na_list]
            return
        if np.allclose(reg.transform(IMC_reg), 0):
            [trial.set_user_attr(k, np.NaN) for k in df_na_list]
            return
            
        # Compute registration metrics
        score = reg.score(IMC_processed, IF_processed)
        
        # Transform and stack images
        if_rescale_shape = (int(IF.shape[2]*if_scale), int(IF.shape[1]*if_scale))
        stack = reg.transform(IMC, np.array([cv2.resize(IF[i], if_rescale_shape, 
                            interpolation=cv2.INTER_LANCZOS4) for i in range(IF.shape[0])]))
        
        # Extract channel-specific stacks
        reg_stack = stack[np.concatenate([
            images['IMC'].c.to_index().isin(registration_channels),
            images['IF'].c.to_index().isin(registration_channels)
        ])]
        corr_stack = stack[np.concatenate([
            images['IMC'].c.to_index().isin(correlation_channels),
            images['IF'].c.to_index().isin(correlation_channels)
        ])]
        
        # Compute correlations over image intersection
        stack_mask = reg.transform(np.ones(IMC_processed.shape), 
                                 np.ones(IF_processed.shape)).sum(0) > 1
        if stack_mask.any():
            stack_image_max_corr = np.nanmax(multi_channel_corr(
                stack[:,stack_mask][0:IMC.shape[0]], 
                stack[:,stack_mask][IMC.shape[0]:]
            ))
            reg_image_max_corr = np.nanmax(multi_channel_corr(
                reg_stack[:,stack_mask][0:IMC_reg.shape[0]], 
                reg_stack[:,stack_mask][IMC_reg.shape[0]:]
            ))
            corr_image_max_corr = np.nanmax(multi_channel_corr(
                corr_stack[:,stack_mask][0:IMC_corr.shape[0]], 
                corr_stack[:,stack_mask][IMC_corr.shape[0]:]
            ))
        else:
            stack_image_max_corr = reg_image_max_corr = corr_image_max_corr = np.nan
            
        # Compute correlations over pixel intersection
        stack_mask = reg.transform(IMC_processed, IF_processed).sum(0) > 1
        if stack_mask.any():
            stack_cell_max_corr = np.nanmax(multi_channel_corr(
                stack[:,stack_mask][0:IMC.shape[0]], 
                stack[:,stack_mask][IMC.shape[0]:]
            ))
            reg_cell_max_corr = np.nanmax(multi_channel_corr(
                reg_stack[:,stack_mask][0:IMC_reg.shape[0]], 
                reg_stack[:,stack_mask][IMC_reg.shape[0]:]
            ))
            corr_cell_max_corr = np.nanmax(multi_channel_corr(
                corr_stack[:,stack_mask][0:IMC_corr.shape[0]], 
                corr_stack[:,stack_mask][IMC_corr.shape[0]:]
            ))
        else:
            stack_cell_max_corr = reg_cell_max_corr = corr_cell_max_corr = np.nan
            
    elif trial.params["registration_target"] == 'IMC':
        raise NotImplementedError("IMC registration target not implemented")
        
    # Set trial attributes
    trial.set_user_attr('registration_matrix', reg.M_)
    trial.set_user_attr('source_sum', score['source_sum'])
    trial.set_user_attr('target_sum', score['target_sum'])
    trial.set_user_attr('logical_and', score['and'])
    trial.set_user_attr('logical_or', score['or'])
    trial.set_user_attr('logical_xor', score['xor'])
    trial.set_user_attr('logical_iou', score['iou'])
    trial.set_user_attr('stack_image_max_corr', stack_image_max_corr)
    trial.set_user_attr('reg_image_max_corr', reg_image_max_corr)
    trial.set_user_attr('corr_image_max_corr', corr_image_max_corr)
    trial.set_user_attr('stack_cell_max_corr', stack_cell_max_corr)
    trial.set_user_attr('reg_cell_max_corr', reg_cell_max_corr)
    trial.set_user_attr('corr_cell_max_corr', corr_cell_max_corr)


def iou_single_objective(trial, images, if_scale, registration_channels, correlation_channels):
    """Objective function that optimizes for IoU (Intersection over Union)."""
    registration_trial(trial, images, if_scale, registration_channels, correlation_channels)
    if np.isnan(trial.user_attrs['reg_image_max_corr']):
        return 0
    return trial.user_attrs['reg_image_max_corr'] * trial.user_attrs['logical_iou']

def iou_multi_objective(trial, images, if_scale, registration_channels, correlation_channels):
    """Multi-objective function that optimizes for both correlation and IoU."""
    registration_trial(trial, images, if_scale, registration_channels, correlation_channels)
    if np.isnan(trial.user_attrs['reg_image_max_corr']):
        return 0, 0
    return trial.user_attrs['reg_image_max_corr'], trial.user_attrs['logical_iou']

# Setup configuration
pairs = pd.read_csv('pairs.csv')
OUT_DIR = 'results/'
PLOTS_DIR = 'plots/'

pairs.set_index(["set_id","if_id","imc_id"], inplace=True, drop=False)
seeds = range(235,238)
samp_dict = {
    'TPESampler': optuna.samplers.TPESampler, 
    'RandomSampler': optuna.samplers.RandomSampler, 
    'GPSampler': optuna.samplers.GPSampler
}
obj_dict = {
    'iou_single_objective': [iou_single_objective, 'maximize'], 
    'iou_multi_objective': [iou_multi_objective, ['maximize','maximize']]
}

targets = expand(["{parent_path}/{if_id}~{imc_id}"], 
                zip, parent_path=[f'{OUT_DIR}/'+x for x in pairs.set_id.to_list()], 
                if_id=pairs.if_id.to_list(), imc_id=pairs.imc_id.to_list())

rule all:
    input: 
        f'{OUT_DIR}/aggregated_study_df.csv'

rule run_optuna: 
    input: ["data/{set_id}/{if_id}.tif", "data/{set_id}/{imc_id}.tiff", 
            "data/{set_id}/IF_panel.csv", "data/{set_id}/IMC_panel.csv"]
    output: OUT_DIR + "/{set_id}/{if_id}~{imc_id}~{objective}~{seed}~{sampler}_study_df.csv"
    run: 
        # Load images
        images = prep_zarr(
            imread(input[0]), imread(input[1]),
            IF_panel=pd.read_csv(input[2], header=None)[0].to_list(),
            IMC_panel=pd.read_csv(input[3], header=None)[0].to_list()
        )
        registration_channels = pairs.loc[wildcards.set_id, wildcards.if_id, wildcards.imc_id]['registration_channels'].split(' ')
        correlation_channels = pairs.loc[wildcards.set_id, wildcards.if_id, wildcards.imc_id]['correlation_channels'].split(' ')
        
        # Setup callbacks
        cbs = [SaveTrialsDFCallback(output[0], anno_dict={
            'objective': wildcards.objective,
            'sampler': wildcards.sampler,
            'seed': wildcards.seed,
            'set_id': wildcards.set_id,
            'if_id': wildcards.if_id,
            'imc_id': wildcards.imc_id
        })]
        
        # Create study
        if wildcards.objective == 'iou_single_objective':
            study = optuna.create_study(
                direction=obj_dict[wildcards.objective][1],
                study_name=f"{wildcards.if_id}~{wildcards.imc_id}",
                sampler=samp_dict[wildcards.sampler](seed=int(wildcards.seed))
            )
        elif wildcards.objective == 'iou_multi_objective':
            study = optuna.create_study(
                directions=obj_dict[wildcards.objective][1],
                study_name=f"{wildcards.if_id}~{wildcards.imc_id}",
                sampler=samp_dict[wildcards.sampler](seed=int(wildcards.seed))
            )
            
        # Add baseline trial
        if_scale = pairs.loc[wildcards.set_id, wildcards.if_id, wildcards.imc_id]['if_scale']
        study.enqueue_trial({
            'IF_binarization_threshold': threshold_otsu(preprocess_if(
                images['IF'][images['IF'].c.to_index().isin(registration_channels)].to_numpy(),
                if_scale, binarize=False, sigma=0
            )),
            'IF_gaussian_sigma': 0,
            'IMC_gaussian_sigma': 0,
            'IMC_binarization_threshold': threshold_otsu(preprocess_imc(
                images['IMC'][images['IMC'].c.to_index().isin(registration_channels)].to_numpy(),
                arcsinh_normalize=False, winsorize_limits=[0.01,0.01], 
                binarize=False, sigma=0
            )),
            'IMC_arcsinh_normalize': False,
            'IMC_winsorization_lower_limit': 0.01,
            'IMC_winsorization_upper_limit': 0.01,
            'registration_target': 'IF',
            'binarize_images': True,
            'registration_max_features': int(1e5),
            'registration_percentile': 0.9
        })
        
        # Run optimization
        study.optimize(
            lambda trial: obj_dict[wildcards.objective][0](
                trial, images, if_scale, registration_channels, correlation_channels
            ),
            n_trials=200,
            callbacks=cbs
        )

rule aggregate:
    input: [
        expand("{x}~{objective}~{seed}~{sampler}_study_df.csv",
               x=targets, objective=['iou_single_objective'], seed=seeds,
               sampler=samp_dict.keys()),
        expand("{x}~{objective}~{seed}~{sampler}_study_df.csv",
               x=targets, objective='iou_multi_objective', seed=seeds,
               sampler=['TPESampler', 'RandomSampler'])
    ]
    output: f"{OUT_DIR}/aggregated_study_df.csv"
    resources: mem_mb=30000
    run: 
        input_files = list(chain.from_iterable({input}))
        input_files = [str(f) for f in input_files]
        study_df = pd.concat([
            pd.read_csv(study_df_path, index_col=0).assign(path=study_df_path)
            for study_df_path in input_files
        ])
        study_df.to_csv(output[0])
